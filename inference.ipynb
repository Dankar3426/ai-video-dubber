{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722df29c",
   "metadata": {},
   "source": [
    "# Deeptune Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a3ccf3",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fa83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import phonemizer\n",
    "import pyannote.audio\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import openai\n",
    "\n",
    "from IPython.display import Audio, Video\n",
    "from attrdict import AttrDict\n",
    "from denoiser import pretrained\n",
    "from denoiser.dsp import convert_audio\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "from munch import Munch\n",
    "from pyannote.audio import Inference, Model, Pipeline\n",
    "from pyannote.core import Segment\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent, split_on_silence\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from vocoder import Generator\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from whisper import *\n",
    "from yaml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fdd494",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ded80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074e8a6",
   "metadata": {},
   "source": [
    "##### Clip Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0675c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_video(video_file_path, video_clip_file_path, clip_length):\n",
    "    video = VideoFileClip(video_file_path).subclip(0, clip_length)\n",
    "    video.write_videofile(video_clip_file_path)\n",
    "\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "video_file_path = os.path.join(current_directory, \"Media/video.mp4\")\n",
    "video_clip_file_path = os.path.join(current_directory, \"Media/video_clip.mp4\")\n",
    "clip_length = 60\n",
    "\n",
    "clip_video(video_file_path, video_clip_file_path, clip_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15aedab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(video_clip_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e6b34c",
   "metadata": {},
   "source": [
    "##### Extract Audio From Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_from_video(video_file_path, audio_file_path):\n",
    "    video = VideoFileClip(video_file_path)\n",
    "    video.audio.write_audiofile(audio_file_path, codec=\"pcm_s32le\")\n",
    "\n",
    "\n",
    "video_file_path = os.path.join(current_directory, \"Media/video_clip.mp4\")\n",
    "audio_file_path = os.path.join(current_directory, \"Media/audio_clip.wav\")\n",
    "\n",
    "extract_audio_from_video(video_file_path, audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07253a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7981e2",
   "metadata": {},
   "source": [
    "##### Separate Vocals And Background Audio\n",
    "Reference Repo: https://github.com/tsurumeso/vocal-remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_vocals_and_background_audio(audio_file_path, output_directory):\n",
    "    command = f\"python3 inference.py --input {audio_file_path} --tta --output_dir {output_directory}\"\n",
    "    parent_directory = os.path.dirname(current_directory)\n",
    "    directory = os.path.join(parent_directory, \"vocal-remover\")\n",
    "    subprocess.run(command, cwd=directory, shell=True)\n",
    "\n",
    "\n",
    "output_directory = os.path.join(current_directory, \"Media\")\n",
    "separate_vocals_and_background_audio(audio_file_path, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ecdacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocals_file_path = os.path.join(current_directory, \"Media/audio_clip_Vocals.wav\")\n",
    "background_audio_file_path = os.path.join(\n",
    "    current_directory, \"Media/audio_clip_Instruments.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c48448",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(vocals_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ddd35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(background_audio_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41812aa",
   "metadata": {},
   "source": [
    "##### Denoise Vocals\n",
    "Reference Repo: https://github.com/facebookresearch/denoiser/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ec2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_vocals(vocals_file_path, denoised_vocals_file_path):\n",
    "    denoiser_model = pretrained.dns64()\n",
    "\n",
    "    wav, sr = torchaudio.load(vocals_file_path)\n",
    "    wav = convert_audio(wav, sr, denoiser_model.sample_rate, denoiser_model.chin)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        denoised = denoiser_model(wav[None])[0]\n",
    "\n",
    "    torchaudio.save(\n",
    "        denoised_vocals_file_path, denoised.cpu(), denoiser_model.sample_rate\n",
    "    )\n",
    "\n",
    "\n",
    "denoised_vocals_file_path = os.path.join(current_directory, \"Media/denoised_vocals.wav\")\n",
    "denoise_vocals(vocals_file_path, denoised_vocals_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(denoised_vocals_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc891ac5",
   "metadata": {},
   "source": [
    "##### Split Audio On Silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76452eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_on_silence(\n",
    "    audio_file_path, output_directory, min_silence_len=1000, silence_thresh=-40\n",
    "):\n",
    "    audio = AudioSegment.from_wav(audio_file_path)\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    non_silent_ranges = detect_nonsilent(audio, min_silence_len, silence_thresh)\n",
    "    segment_timestamps = []\n",
    "    speech_start_times = []\n",
    "    last_end = 0\n",
    "    speech_count = 0\n",
    "\n",
    "    for i, (start_i, end_i) in enumerate(non_silent_ranges):\n",
    "        segment = audio[start_i:end_i]\n",
    "        segment.export(\n",
    "            os.path.join(output_directory, f\"speech_audio_segment_{i}.wav\"),\n",
    "            format=\"wav\",\n",
    "        )\n",
    "\n",
    "        start_s = start_i / 1000\n",
    "        end_s = end_i / 1000\n",
    "\n",
    "        silence_start_s = last_end / 1000\n",
    "        silence_end_s = start_s\n",
    "        if silence_start_s < silence_end_s:\n",
    "            segment_timestamps.append((silence_start_s, silence_end_s, \"silence\"))\n",
    "\n",
    "        segment_timestamps.append((start_s, end_s, \"speech\"))\n",
    "        speech_start_times.append(start_s)\n",
    "        speech_count += 1\n",
    "\n",
    "        last_end = end_i\n",
    "\n",
    "    total_length_s = len(audio) / 1000\n",
    "    if last_end < total_length_s:\n",
    "        segment_timestamps.append((last_end / 1000, total_length_s, \"silence\"))\n",
    "\n",
    "    return segment_timestamps, speech_count, speech_start_times\n",
    "\n",
    "\n",
    "output_directory = os.path.join(current_directory, \"Media\")\n",
    "segment_timestamps, speech_count, speech_start_times = split_audio_on_silence(\n",
    "    denoised_vocals_file_path, output_directory\n",
    ")\n",
    "segment_timestamps, speech_count, speech_start_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_segment_file_path = os.path.join(output_directory, \"speech_audio_segment_0.wav\")\n",
    "Audio(audio_segment_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896cd68b",
   "metadata": {},
   "source": [
    "##### Speech-To-Text\n",
    "Reference Repo: https://github.com/openai/whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(vocals_file_path):\n",
    "    audio_file = open(vocals_file_path, \"rb\")\n",
    "    result = openai.Audio.transcribe(\n",
    "        \"whisper-1\", audio_file, response_format=\"verbose_json\", temperature=0\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i in tqdm(range(speech_count)):\n",
    "    segment_vocals_vocals_file_path = os.path.join(\n",
    "        current_directory, f\"Media/speech_audio_segment_{i}.wav\"\n",
    "    )\n",
    "    results.append(transcribe(segment_vocals_vocals_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b108684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "starts = []\n",
    "ends = []\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    if result[\"text\"] != \"\":\n",
    "        segments = result[\"segments\"]\n",
    "        for segment in segments:\n",
    "            texts.append(segment[\"text\"].strip())\n",
    "            starts.append(round(speech_start_times[i] + segment[\"start\"], 2))\n",
    "            ends.append(round(speech_start_times[i] + segment[\"end\"], 2))\n",
    "\n",
    "df = pd.DataFrame((texts, starts, ends), index=(\"texts\", \"starts\", \"ends\")).T\n",
    "df = df[df[\"texts\"].str.contains(r\"[a-zA-Z]\", regex=True)]\n",
    "df[\"durations\"] = df.ends - df.starts\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e135a",
   "metadata": {},
   "source": [
    "##### Audio Embeddings\n",
    "Reference Repo: https://github.com/pyannote/pyannote-audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_AUTH_TOKEN = AUTH_TOKEN\n",
    "pyannote_embedding_model = Model.from_pretrained(\n",
    "    \"pyannote/embedding\", use_auth_token=USE_AUTH_TOKEN\n",
    ")\n",
    "inference = Inference(pyannote_embedding_model, window=\"whole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310cef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_embeddings(audio_file_path):\n",
    "    audio_embeddings = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        start = row[\"starts\"]\n",
    "        end = row[\"ends\"]\n",
    "        excerpt = Segment(start, end)\n",
    "        audio_embedding = inference.crop(audio_file_path, excerpt)\n",
    "        audio_embeddings.append(audio_embedding)\n",
    "\n",
    "    return np.array(audio_embeddings)\n",
    "\n",
    "\n",
    "audio_embeddings = get_audio_embeddings(denoised_vocals_file_path)\n",
    "audio_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afb604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(embeddings, n_clusters):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "    scaled_embeddings.shape\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(scaled_embeddings)\n",
    "\n",
    "    return kmeans.labels_\n",
    "\n",
    "\n",
    "n_clusters = 5\n",
    "labels = cluster(audio_embeddings, n_clusters)\n",
    "df[\"audio_clusters\"] = labels\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c84708",
   "metadata": {},
   "source": [
    "##### Text Embeddings\n",
    "Reference: https://platform.openai.com/docs/api-reference/chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    text = row[\"texts\"]\n",
    "    response = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n",
    "    text_embedding = response.data[0].embedding\n",
    "    text_embeddings.append(text_embedding)\n",
    "\n",
    "text_embeddings = np.array(text_embeddings)\n",
    "print(text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = cluster(text_embeddings, n_clusters)\n",
    "df[\"text_clusters\"] = labels\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa80d43",
   "metadata": {},
   "source": [
    "##### Speaker Diarization\n",
    "Reference: https://platform.openai.com/docs/api-reference/chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_transcript = \"\"\n",
    "\n",
    "\n",
    "def format_time(time_float):\n",
    "    minutes = int(time_float // 60)\n",
    "    seconds = int(time_float % 60)\n",
    "    fraction = int((time_float * 100) % 100)\n",
    "    return f\"{minutes:02d}:{seconds:02d}.{fraction:02d}\"\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    start_time = row[\"starts\"]\n",
    "    end_time = row[\"ends\"]\n",
    "    transcription = row[\"texts\"]\n",
    "    audio_cluster = row[\"audio_clusters\"]\n",
    "    text_cluster = row[\"text_clusters\"]\n",
    "\n",
    "    start_time_str = format_time(start_time)\n",
    "    end_time_str = format_time(end_time)\n",
    "\n",
    "    output_string = f'Row {index} - {start_time_str}-{end_time_str} - Audio Cluster {audio_cluster} – Text Cluster {text_cluster} – \"{transcription}\"'\n",
    "\n",
    "    formatted_transcript += output_string + \"\\n\"\n",
    "\n",
    "formatted_transcript = formatted_transcript.rstrip(\"\\n\")\n",
    "print(formatted_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f52eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = (\n",
    "    \"Return in a list one speaker tag for each row in the following video transcript:\\n\"\n",
    "    + formatted_transcript\n",
    ")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa248156",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": content}], temperature=0\n",
    ")\n",
    "\n",
    "speaker_string = completion.choices[0].message.content\n",
    "speaker_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f113e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_list = ast.literal_eval(speaker_string)\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoded_speakers = encoder.fit_transform(speaker_list)\n",
    "df[\"speakers\"] = encoded_speakers\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a255bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_speaker = None\n",
    "current_row = None\n",
    "new_rows = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row[\"speakers\"] == current_speaker:\n",
    "        current_row[\"texts\"] += \" \" + row[\"texts\"]\n",
    "        current_row[\"ends\"] = row[\"ends\"]\n",
    "        current_row[\"durations\"] += row[\"durations\"]\n",
    "    else:\n",
    "        if current_row is not None:\n",
    "            new_rows.append(current_row)\n",
    "\n",
    "        current_speaker = row[\"speakers\"]\n",
    "        current_row = row.copy()\n",
    "\n",
    "if current_row is not None:\n",
    "    new_rows.append(current_row)\n",
    "\n",
    "diarized_df = pd.DataFrame(new_rows).reset_index(drop=True)\n",
    "diarized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2084a293",
   "metadata": {},
   "source": [
    "##### Translation\n",
    "Reference: https://platform.openai.com/docs/api-reference/chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfec5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = (\n",
    "    \"Return an idiomatic English translation of the following video transcript:\\n\"\n",
    "    + \"\\n\".join(diarized_df.texts)\n",
    ")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b26a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": content}], temperature=0\n",
    ")\n",
    "\n",
    "translation = completion.choices[0].message.content\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae096c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarized_df[\"translations\"] = translation.split(\"\\n\")\n",
    "diarized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f99a8",
   "metadata": {},
   "source": [
    "##### Collect Reference Speech For StyleTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio(audio_file_path, start_time, end_time):\n",
    "    audio = AudioSegment.from_wav(audio_file_path)\n",
    "    start_time = start_time * 1000\n",
    "    end_time = end_time * 1000\n",
    "    return audio[start_time:end_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389d5974",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_audio_dict = {}\n",
    "\n",
    "for index, row in diarized_df.iterrows():\n",
    "    start = row[\"starts\"]\n",
    "    end = row[\"ends\"]\n",
    "    speaker = row[\"audio_clusters\"]\n",
    "\n",
    "    extracted_audio_segment = extract_audio(vocals_file_path, start, end)\n",
    "\n",
    "    if speaker in speaker_audio_dict:\n",
    "        speaker_audio_dict[speaker] += extracted_audio_segment\n",
    "    else:\n",
    "        speaker_audio_dict[speaker] = extracted_audio_segment\n",
    "\n",
    "    reference_file_path = os.path.join(\n",
    "        current_directory, f\"Media/reference_{index}.wav\"\n",
    "    )\n",
    "    extracted_audio_segment.export(reference_file_path)\n",
    "\n",
    "for speaker, audio_segment in speaker_audio_dict.items():\n",
    "    speaker_reference_file_path = os.path.join(\n",
    "        current_directory, f\"Media/speaker_reference_{speaker}.wav\"\n",
    "    )\n",
    "    audio_segment.export(speaker_reference_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42376ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(reference_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ce9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(speaker_reference_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed54e054",
   "metadata": {},
   "source": [
    "##### Setup Text-To-Speech Models\n",
    "Reference Repo: https://github.com/yl4579/StyleTTS\n",
    "\n",
    "Pre-trained StyleTTS and Hifi-GAN on LibriTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43486ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pad = \"$\"\n",
    "_punctuation = ';:,.!?¡¿—…\"«»“” '\n",
    "_letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "_letters_ipa = \"ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\"\n",
    "\n",
    "\n",
    "symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)\n",
    "\n",
    "dicts = {}\n",
    "for i in range(len((symbols))):\n",
    "    dicts[symbols[i]] = i\n",
    "\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self, dummy=None):\n",
    "        self.word_index_dictionary = dicts\n",
    "\n",
    "    def __call__(self, text):\n",
    "        indexes = []\n",
    "        for char in text:\n",
    "            try:\n",
    "                indexes.append(self.word_index_dictionary[char])\n",
    "            except KeyError:\n",
    "                print(char)\n",
    "        return indexes\n",
    "\n",
    "\n",
    "textcleaner = TextCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300\n",
    ")\n",
    "mean, std = -4, 4\n",
    "\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "    mask = (\n",
    "        torch.arange(lengths.max())\n",
    "        .unsqueeze(0)\n",
    "        .expand(lengths.shape[0], -1)\n",
    "        .type_as(lengths)\n",
    "    )\n",
    "    mask = torch.gt(mask + 1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "\n",
    "def compute_style(ref_dicts):\n",
    "    reference_embeddings = {}\n",
    "    for key, path in ref_dicts.items():\n",
    "        wave, sr = librosa.load(path, sr=24000)\n",
    "        audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "        if sr != 24000:\n",
    "            audio = librosa.resample(audio, sr, 24000)\n",
    "        mel_tensor = preprocess(audio).to(device)\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                ref = model.style_encoder(mel_tensor.unsqueeze(1))\n",
    "            reference_embeddings[key] = (ref.squeeze(1), audio)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return reference_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81657e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_phonemizer = phonemizer.backend.EspeakBackend(\n",
    "    language=\"en-us\", preserve_punctuation=True, with_stress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = None\n",
    "\n",
    "\n",
    "def load_checkpoint(filepath, device):\n",
    "    assert os.path.isfile(filepath)\n",
    "    print(\"Loading '{}'\".format(filepath))\n",
    "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
    "    print(\"Complete.\")\n",
    "    return checkpoint_dict\n",
    "\n",
    "\n",
    "def scan_checkpoint(cp_dir, prefix):\n",
    "    pattern = os.path.join(cp_dir, prefix + \"*\")\n",
    "    cp_list = glob.glob(pattern)\n",
    "    if len(cp_list) == 0:\n",
    "        return \"\"\n",
    "    return sorted(cp_list)[-1]\n",
    "\n",
    "\n",
    "cp_g = scan_checkpoint(\"Vocoder/LibriTTS/\", \"g_\")\n",
    "\n",
    "config_file = os.path.join(os.path.split(cp_g)[0], \"config.json\")\n",
    "with open(config_file) as f:\n",
    "    data = f.read()\n",
    "json_config = json.loads(data)\n",
    "h = AttrDict(json_config)\n",
    "\n",
    "device = torch.device(device)\n",
    "generator = Generator(h).to(device)\n",
    "\n",
    "state_dict_g = load_checkpoint(cp_g, device)\n",
    "generator.load_state_dict(state_dict_g[\"generator\"])\n",
    "generator.eval()\n",
    "generator.remove_weight_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a50fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./Models/LibriTTS/epoch_2nd_00050.pth\"\n",
    "model_config_path = \"./Models/LibriTTS/config.yml\"\n",
    "\n",
    "config = yaml.safe_load(open(model_config_path))\n",
    "\n",
    "ASR_config = config.get(\"ASR_config\", False)\n",
    "ASR_path = config.get(\"ASR_path\", False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "F0_path = config.get(\"F0_path\", False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "model = build_model(Munch(config[\"model_params\"]), text_aligner, pitch_extractor)\n",
    "\n",
    "params = torch.load(model_path, map_location=\"cpu\")\n",
    "params = params[\"net\"]\n",
    "for key in model:\n",
    "    if key in params:\n",
    "        if not \"discriminator\" in key:\n",
    "            print(\"%s loaded\" % key)\n",
    "            model[key].load_state_dict(params[key])\n",
    "_ = [model[key].eval() for key in model]\n",
    "_ = [model[key].to(device) for key in model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9439f4",
   "metadata": {},
   "source": [
    "##### Synthesize Speech\n",
    "Reference Repo: https://github.com/yl4579/StyleTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_speech(\n",
    "    text,\n",
    "    style_reference,\n",
    "    speaker_reference,\n",
    "    audio_reference,\n",
    "    output_filename,\n",
    "    target_duration,\n",
    "):\n",
    "    if target_duration < 1:\n",
    "        ref_dicts = {\"single_speaker\": speaker_reference}\n",
    "    else:\n",
    "        ref_dicts = {\"single_speaker\": style_reference}\n",
    "\n",
    "    reference_embeddings = compute_style(ref_dicts)\n",
    "\n",
    "    if len(reference_embeddings) == 0:\n",
    "        ref_dicts = {\"single_speaker\": audio_reference}\n",
    "        reference_embeddings = compute_style(ref_dicts)\n",
    "\n",
    "    ref, _ = reference_embeddings[\"single_speaker\"]\n",
    "    s = ref.squeeze(1)\n",
    "    style = s\n",
    "\n",
    "    ps = global_phonemizer.phonemize([text])\n",
    "    tokens = textcleaner(ps[0])\n",
    "    tokens.insert(0, 0)\n",
    "    tokens.append(0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "        m = length_to_mask(input_lengths).to(device)\n",
    "        t_en = model.text_encoder(tokens, input_lengths, m)\n",
    "        d = model.predictor.text_encoder(t_en, style, input_lengths, m)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        target_frames = int((target_duration * 1000) / 12.5)\n",
    "        scaling_factor = target_frames / pred_dur.sum().item() / 2\n",
    "        scaled_pred_dur = (pred_dur * scaling_factor).round().clamp(min=1)\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(scaled_pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame : c_frame + int(scaled_pred_dur[i].data)] = 1\n",
    "            c_frame += int(scaled_pred_dur[i].data)\n",
    "\n",
    "        en = d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        out = model.decoder(\n",
    "            (t_en @ pred_aln_trg.unsqueeze(0).to(device)),\n",
    "            F0_pred,\n",
    "            N_pred,\n",
    "            ref.squeeze().unsqueeze(0),\n",
    "        )\n",
    "        c = out.squeeze()\n",
    "        y_g_hat = generator(c.unsqueeze(0))\n",
    "        y_out = y_g_hat.squeeze().cpu().numpy()\n",
    "\n",
    "        sf.write(output_filename, y_out, 24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9737b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in tqdm(diarized_df.iterrows(), total=len(diarized_df)):\n",
    "    translation = row[\"translations\"]\n",
    "    duration = row[\"durations\"]\n",
    "    speaker = row[\"audio_clusters\"]\n",
    "\n",
    "    style_reference = os.path.join(current_directory, f\"Media/reference_{i}.wav\")\n",
    "    speaker_reference = os.path.join(\n",
    "        current_directory, f\"Media/speaker_reference_{speaker}.wav\"\n",
    "    )\n",
    "    output_filename = os.path.join(current_directory, f\"Media/speech_{i}.wav\")\n",
    "\n",
    "    synthesize_speech(\n",
    "        translation,\n",
    "        style_reference,\n",
    "        speaker_reference,\n",
    "        vocals_file_path,\n",
    "        output_filename,\n",
    "        duration,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccc630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(f\"Media/speech_{0}.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8048dc31",
   "metadata": {},
   "source": [
    "##### Add Generated Speech To Background Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa65f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_files = [\n",
    "    {\"file\": f\"Media/speech_{i}.wav\", \"start_time\": diarized_df.starts[i] * 1000}\n",
    "    for i in range(diarized_df.starts.shape[0])\n",
    "]\n",
    "\n",
    "background = AudioSegment.from_wav(background_audio_file_path)\n",
    "\n",
    "for voice in voice_files:\n",
    "    voice_audio = AudioSegment.from_wav(voice[\"file\"])\n",
    "    start_time = voice[\"start_time\"]\n",
    "\n",
    "    background = background.overlay(voice_audio, position=start_time)\n",
    "\n",
    "final_audio_output_file_path = os.path.join(\n",
    "    current_directory, \"Media/final_audio_output.wav\"\n",
    ")\n",
    "background.export(final_audio_output_file_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6792e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(final_audio_output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64177224",
   "metadata": {},
   "source": [
    "##### Write Audio To Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47faf242",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_input_path = os.path.join(current_directory, \"Media/video_clip.mp4\")\n",
    "audio_input_path = os.path.join(current_directory, \"Media/final_audio_output.wav\")\n",
    "video_output_path = os.path.join(current_directory, \"Media/final_video_output.mp4\")\n",
    "\n",
    "video = VideoFileClip(video_input_path)\n",
    "new_audio = AudioFileClip(audio_input_path)\n",
    "\n",
    "video = video.set_audio(new_audio)\n",
    "\n",
    "video.write_videofile(video_output_path, codec=\"libx264\", audio_codec=\"aac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773a1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_display_video_output_path = os.path.join(\n",
    "    current_directory, \"Media/notebook_display_video_output.mp4\"\n",
    ")\n",
    "video.write_videofile(notebook_display_video_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a32ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(notebook_display_video_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3c2df",
   "metadata": {},
   "source": [
    "##### Moving Forward\n",
    "\n",
    "- use facial expressions\n",
    "- facial recognition\n",
    "- dig deeper Wav2Lip\n",
    "- cleaner snippets\n",
    "- better timestamps\n",
    "- explore multimodal approaches\n",
    "- generate metadata (number of speakers)\n",
    "- better manage short audio segments\n",
    "- ensembling\n",
    "- facebook multimodal translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
